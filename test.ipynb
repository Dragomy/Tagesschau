{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef3a32b-f2cf-49e2-98b6-7cfdfd0209ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: This script is being currently worked on.\n",
    "\n",
    "from read_data import data_to_dataframe\n",
    "import pandas as pd\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168745fb-1194-4cf4-b0b7-18f8dde8d354",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_to_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# All files get converted into a dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m daily_news \u001b[38;5;241m=\u001b[39m \u001b[43mdata_to_dataframe\u001b[49m() \n\u001b[1;32m      4\u001b[0m known_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msophoraId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternalId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteaserImage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdateCheckUrl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracking\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstSentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetailsweb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshareURL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeotags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregionId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregionIds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreakingNews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mressort\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrandingImage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malttext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopyright\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Should there be a missing column that should not raise problems \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_to_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# All files get converted into a dataframe\n",
    "daily_news = data_to_dataframe() \n",
    "\n",
    "known_columns = [\"sophoraId\", \"externalId\", \"title\", \"date\",\n",
    "                 \"teaserImage\", \"tags\", \"updateCheckUrl\", \"tracking\",\n",
    "                 \"topline\", \"firstSentence\", \"details\", \"detailsweb\",\n",
    "                 \"shareURL\", \"geotags\", \"regionId\", \"regionIds\",\n",
    "                 \"type\", \"breakingNews\", \"ressort\", \"brandingImage\",\n",
    "                 \"streams\", \"alttext\", \"copyright\", \"comments\"]\n",
    "\n",
    "# Should there be a missing column that should not raise problems \n",
    "available_columns = [col for col in known_columns if col in daily_news.columns]\n",
    "daily_news_passed = daily_news[available_columns]\n",
    "\n",
    "# Check if there are unknown collumns \n",
    "new_columns = [column for column in daily_news.columns if column not in known_columns]\n",
    "\n",
    "# Should an unknown collumn be found save it in a dataframe with its sophoraId and date as Identifier\n",
    "if new_columns:\n",
    "    daily_news_failed = daily_news[[\"sophoraId\", \"date\"] + new_columns]\n",
    "\n",
    "# Split daily into first and last\n",
    "first = daily_news_passed.drop_duplicates(subset = \"sophoraId\", keep = \"first\")\n",
    "last = daily_news_passed.drop_duplicates(subset = \"sophoraId\", keep = \"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfee3a5-a844-427a-9334-6e59daa14d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements: \n",
    "def flatten_teaserImage(teaser_image):\n",
    "    if isinstance(teaser_image, dict):\n",
    "        flat_data = {\n",
    "            'teaserImage_alttext': teaser_image.get('alttext', ''),\n",
    "            'type': teaser_image.get('type', ''),\n",
    "        }\n",
    "        image_variants = teaser_image.get('imageVariants', {})\n",
    "        if isinstance(image_variants, dict):\n",
    "            for key, value in image_variants.items():\n",
    "                flat_data[f'imageVariants_{key}'] = value\n",
    "        return flat_data\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6f1fe3-1f34-4b6b-84ce-dfa9fe40f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag(tmp_df):\n",
    "    if isinstance(tmp_df, dict):  \n",
    "        return tmp_df.get('tag')  \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398226dd-8f47-4d5a-84d6-ce86d4a845df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tracking(df):  \n",
    "    if 'tracking' in df.columns and df['tracking'].notna().any():\n",
    "        \n",
    "        tracking_df = df['tracking'].apply(lambda x: pd.Series(x[0]) if isinstance(x, list) and len(x) > 0 else pd.Series())\n",
    "        tracking_df.columns = [f'tracking_{col}' for col in tracking_df.columns]\n",
    "        df = pd.concat([df.drop(columns=['tracking']), tracking_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84877d48-f234-4be6-9437-5e7dcde2135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, name):\n",
    "    file_name = f'{name}.parquet.gzip'\n",
    "    df.to_parquet(path = file_name, engine = \"pyarrow\", compression = \"gzip\", index = False)\n",
    "    print(f'Saved {file_name}')\n",
    "    \n",
    "# Before appending them to any older files check for conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c79192-6a6a-48ea-b3f3-5fb3fcd51184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since everything that follows is for both first and last we write a function\n",
    "def flatten_and_save(df, df_name):\n",
    "    \n",
    "    geotags = df[[\"sophoraId\", \"geotags\"]].copy()\n",
    "    geotags_exploded = geotags.explode(\"geotags\")\n",
    "    save_dataframe(geotags_exploded, f'geotags_{df_name}')\n",
    "\n",
    "    regionIds = df[[\"sophoraId\", \"regionIds\"]].copy()\n",
    "    regionIds_exploded = regionIds.explode(\"regionIds\")\n",
    "    save_dataframe(regionIds_exploded, f'regionIds_{df_name}')\n",
    "\n",
    "    teaserImage = df[[\"sophoraId\", \"teaserImage\"]].copy()\n",
    "    flattened_teaserImage = teaserImage[\"teaserImage\"].apply(flatten_teaserImage)\n",
    "    teaserImage_df = pd.DataFrame(flattened_teaserImage.tolist())\n",
    "    teaserImage_combined = pd.concat([teaserImage.drop(columns=['teaserImage']), teaserImage_df], axis=1)\n",
    "    save_dataframe(teaserImage_combined, f'teaserImage_{df_name}')\n",
    "\n",
    "    tags = df[[\"sophoraId\", \"tags\"]].copy()\n",
    "    tags_exploded = tags.explode(\"tags\")\n",
    "    tags_exploded[\"tags\"] = tags_exploded[\"tags\"].apply(extract_tag)\n",
    "    save_dataframe(tags_exploded, f'tags_{df_name}')\n",
    "\n",
    "    tracking = df[[\"sophoraId\", \"tracking\"]].copy()\n",
    "    tracking_flattened = flatten_tracking(tracking)\n",
    "    save_dataframe(tracking_flattened, f'tracking_{df_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10ce562-bf8c-4c75-806d-1fb337980a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved geotags_first.parquet.gzip\n",
      "Saved regionIds_first.parquet.gzip\n",
      "Saved teaserImage_first.parquet.gzip\n",
      "Saved tags_first.parquet.gzip\n",
      "Saved tracking_first.parquet.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6375/1352840120.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(file_name, \"pyarrow\", \"gzip\", index=False)\n",
      "/tmp/ipykernel_6375/1352840120.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(file_name, \"pyarrow\", \"gzip\", index=False)\n",
      "/tmp/ipykernel_6375/1352840120.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(file_name, \"pyarrow\", \"gzip\", index=False)\n",
      "/tmp/ipykernel_6375/1352840120.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(file_name, \"pyarrow\", \"gzip\", index=False)\n",
      "/tmp/ipykernel_6375/1352840120.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(file_name, \"pyarrow\", \"gzip\", index=False)\n"
     ]
    }
   ],
   "source": [
    "test = flatten_and_save(first,\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e58bb4-c183-40e2-b041-51731876a9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
