{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8834f4-f8e6-406b-8c9c-6d74bb67f715",
   "metadata": {},
   "source": [
    "#### The data shall be saved in the Apache Parquet file format for ease of use and time efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a09e37-0216-4e6d-8ea2-57ccbc4d665b",
   "metadata": {},
   "source": [
    "This is a relatively complex solution to the problem I will take the easy route it should be easier to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9ae5a-03aa-492e-ba1b-cc410772c777",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133dffaa-d9d9-41f1-b2d0-be09ea5daf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import data_to_dataframe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2c11d8-df63-4591-841d-22e43a42d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab882d5-9d97-4b4d-8087-b2acbd0cfb14",
   "metadata": {},
   "source": [
    "# Pandas Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63992a2-17c2-47e4-8d08-a368e8895150",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80ccd19-da9c-4a62-ad88-055204911584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b3928-8e69-42bb-bcb8-be493fe86089",
   "metadata": {},
   "source": [
    "# Constructing the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9e09a4-afee-490e-95b9-2750d543a978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dragomy/Documents/Jupyter/oneWeek/News_2024-06-17_04-35-06.json.gz\r"
     ]
    }
   ],
   "source": [
    "data_directory = '/home/dragomy/Documents/Jupyter/oneWeek'\n",
    "df = data_to_dataframe(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349129-d691-4d4b-8d73-65e520f9fc7b",
   "metadata": {},
   "source": [
    "# Flatten the original DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593091a6-2306-4640-8fc4-377faef6b1bb",
   "metadata": {},
   "source": [
    "#### For the flattening we convert our df to the second normalform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c26b779-5d8d-4a85-96f0-d2777d5cea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basics = df[[\"sophoraId\",\"title\",\"date\",\"type\",\"breakingNews\",\"ressort\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594ebba-e002-4694-af0f-d4d2db311cbb",
   "metadata": {},
   "source": [
    "The basics collumn does not contain any collumns with nested datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e7f35a-9d94-4b41-a08c-3353f7312276",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = df[[\"sophoraId\",\"geotags\",\"regionId\",\"regionIds\"]]\n",
    "location = location.explode('geotags').explode('regionIds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae8a6d-dd8c-47d7-8159-dc4251593765",
   "metadata": {},
   "source": [
    "location has several lists that need to be flattened luckily pandas has a function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17439b42-9975-4ad6-b28a-7a164582c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = df[[\"sophoraId\",\"teaserImage\",\"updateCheckUrl\",\"details\",\"detailsweb\",\"shareURL\"]]\n",
    "\n",
    "def flatten_teaser_image(teaser_image):\n",
    "    if isinstance(teaser_image, dict):\n",
    "        flat_data = {\n",
    "            'teaserImage_alttext': teaser_image.get('alttext', ''),\n",
    "            'type': teaser_image.get('type', ''),\n",
    "        }\n",
    "        image_variants = teaser_image.get('imageVariants', {})\n",
    "        if isinstance(image_variants, dict):\n",
    "            for key, value in image_variants.items():\n",
    "                flat_data[f'imageVariants_{key}'] = value\n",
    "        return flat_data\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "flattened_teaser_images = links['teaserImage'].apply(flatten_teaser_image)\n",
    "flattened_df = pd.DataFrame(flattened_teaser_images.tolist())\n",
    "links = pd.concat([links.drop(columns=['teaserImage']), flattened_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab65252-cac9-4a75-a401-08d1ad6066af",
   "metadata": {},
   "source": [
    "links has a collumn containing a dictionary so this takes the keys and uses them as collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d22d35a-f012-4dd6-b544-9f3f0826345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df[[\"sophoraId\",\"tags\"]]\n",
    "tags = tags.explode('tags')\n",
    "\n",
    "def extract_tag(tmp_df):\n",
    "    if isinstance(tmp_df, dict):  \n",
    "        return tmp_df.get('tag')  \n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "tags['tags'] = tags['tags'].apply(extract_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448869c9-e7f2-4ec4-9478-685e9278429f",
   "metadata": {},
   "source": [
    "tags contains a list of dictionarys that always have tags as a key and the value is a string so i extract the string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8823f1b3-1113-40d2-82a1-dc7128495c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "other = df[[\"sophoraId\",\"tracking\",\"topline\",\"firstSentence\",\"brandingImage\",\"streams\",\"alttext\",\"copyright\",\"comments\"]]\n",
    "\n",
    "def normalize_tracking_column(df):\n",
    "    \n",
    "    if 'tracking' in df.columns and df['tracking'].notna().any():\n",
    "        \n",
    "        tracking_df = df['tracking'].apply(lambda x: pd.Series(x[0]) if isinstance(x, list) and len(x) > 0 else pd.Series())\n",
    "        tracking_df.columns = [f'tracking_{col}' for col in tracking_df.columns]\n",
    "        df = pd.concat([df.drop(columns=['tracking']), tracking_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "other = normalize_tracking_column(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecc107-33a7-4b8a-8eee-d13fc5ef0149",
   "metadata": {},
   "source": [
    "other contains the collumns i dont realy know what to do with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff3f0c-a961-498c-a86a-41c75d173e6d",
   "metadata": {},
   "source": [
    "# Save as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fe8398-755a-4e15-bcb5-509bc6462c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, df_name):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    \n",
    "    parquet_name = f\"{df_name}_{current_datetime.strftime('%Y-%m-%d_%H-%M-%S')}.parquet.gzip\"\n",
    "    \n",
    "    df.to_parquet(parquet_name, \"pyarrow\", \"gzip\")\n",
    "    print(f\"DataFrame saved as: {parquet_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221d1652-3927-4734-8306-07ae01c8a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_to_parquet():\n",
    "    save_parquet(basics, \"basics\")\n",
    "    save_parquet(location, \"location\")\n",
    "    save_parquet(links, \"links\")\n",
    "    save_parquet(tags, \"tags\")\n",
    "    save_parquet(other, \"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a584ac65-590f-4f38-ac97-8b9ad2950e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37452/1865536185.py:6: FutureWarning: Starting with pandas version 3.0 all arguments of to_parquet except for the argument 'path' will be keyword-only.\n",
      "  df.to_parquet(parquet_name, \"pyarrow\", \"gzip\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as: basics_2024-07-19_15-02-16.parquet.gzip\n",
      "DataFrame saved as: location_2024-07-19_15-02-16.parquet.gzip\n",
      "DataFrame saved as: links_2024-07-19_15-02-17.parquet.gzip\n",
      "DataFrame saved as: tags_2024-07-19_15-02-19.parquet.gzip\n",
      "DataFrame saved as: other_2024-07-19_15-02-20.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "save_all_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e00e4-eaaa-44be-98da-47ad7b5d7408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
